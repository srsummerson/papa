{
 "metadata": {
  "name": "",
  "signature": "sha256:3fee5f39ce10851eff2ae999b12028df0f55abc69b3b3dfb747c7e7674d7b807"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Uses data from Feb 10 - 25.  Using GLM with binary classifer and Logit model fits agree.  However, using sklearn's LogisticRegression gives different parameter coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd C:\\Users\\Samantha\\Dropbox\\Carmena Lab\\Papa\\hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Users\\Samantha\\Dropbox\\Carmena Lab\\Papa\\hdf\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "import scipy\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "import tables\n",
      "from pylab import rcParams\n",
      "import pandas as pd\n",
      "from patsy import dmatrices\n",
      "from sklearn.linear_model import LogisticRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdf_list = ['papa20150210_13.hdf','papa20150211_11.hdf','papa20150214_18.hdf','papa20150216_05.hdf','papa20150218_04.hdf',\n",
      "            'papa20150219_09.hdf','papa20150223_02.hdf','papa20150224_02.hdf','papa20150303_03.hdf','papa20150306_07.hdf',\n",
      "            'papa20150309_04.hdf']\n",
      "#hdf_list with days where values weren't learned in Block A excluded\n",
      "#hdf_list = ['papa20150211_11.hdf','papa20150214_18.hdf','papa20150216_05.hdf','papa20150218_04.hdf',\n",
      "#            'papa20150219_09.hdf','papa20150223_02.hdf','papa20150224_02.hdf','papa20150303_03.hdf','papa20150306_07.hdf']\n",
      "\n",
      "counter = 0\n",
      "it_side = []\n",
      "it_reward = []\n",
      "fc_low_side = []\n",
      "fc_target = []\n",
      "for name in hdf_list:\n",
      "    hdf = tables.openFile(name)\n",
      "\n",
      "    # states\n",
      "    state = hdf.root.task_msgs[:]['msg']\n",
      "    state_time = hdf.root.task_msgs[:]['time']\n",
      "    # target info: (offset, probability of reward, left/right)\n",
      "    targetH = hdf.root.task[:]['targetH']\n",
      "    targetL = hdf.root.task[:]['targetL']\n",
      "    # reward schedules\n",
      "    reward_scheduleH = hdf.root.task[:]['reward_scheduleH']\n",
      "    reward_scheduleL = hdf.root.task[:]['reward_scheduleL']\n",
      "    # instructed (1) or free-choice (2) trial \n",
      "    trial_type = hdf.root.task[:]['target_index']\n",
      "\n",
      "    ind_wait_states = np.ravel(np.nonzero(state == 'wait'))\n",
      "    ind_target_states = np.ravel(np.nonzero(state == 'target'))\n",
      "    ind_check_reward_states = np.ravel(np.nonzero(state == 'check_reward'))\n",
      "    num_successful_trials = ind_check_reward_states.size\n",
      "    target_times = state_time[ind_target_states]\n",
      "    # creates vector same size of state vectors for comparison. instructed (0) and free-choice (1)\n",
      "    instructed_or_freechoice = trial_type[state_time[ind_check_reward_states]]  \n",
      "    num_free_choice_trials = sum(instructed_or_freechoice) - num_successful_trials\n",
      "\n",
      "    # creates vector of same size of target info: maxtrix of num_successful_trials x 3; (position_offset, reward_prob, left0/right1)\n",
      "    targetH_info = targetH[state_time[ind_target_states]]\n",
      "    targetL_info = targetL[state_time[ind_target_states]]\n",
      "    #counter = 0\n",
      "    #it_side = []\n",
      "    #it_reward = []\n",
      "    #fc_low_side = []\n",
      "    #fc_target = []\n",
      "    \n",
      "    for i in range(200,num_successful_trials):\n",
      "    #for i in range(200,300):\n",
      "        target_state = state[ind_check_reward_states[i] - 2]\n",
      "        #trial = trial_type[state_time[ind_check_reward_states[i] -2]]\n",
      "        trial = instructed_or_freechoice[i]\n",
      "        previous_trial = instructed_or_freechoice[i-1]\n",
      "        if target_state == 'hold_targetL':\n",
      "            target_all_block3 = 1\n",
      "        else:\n",
      "            target_all_block3 = 0  # rather than 2, indicates low value target was not selected\n",
      "\n",
      "        if (trial == 2) and (previous_trial ==1):\n",
      "            it_side.append(targetL_info[i-1][2])   # postion info for previous instructed low value target\n",
      "            it_reward.append(state[ind_check_reward_states[i-1]+1] == 'reward')  # reward for previous instructed low value target\n",
      "            fc_low_side.append(targetL_info[i][2]) # what side low value target is presented on in free choice trial\n",
      "            fc_target.append(target_all_block3)  # what target is selected: 1 = low value, 2 = high value\n",
      "            #it_side = np.append(it_side,targetL_info[i-1][2])   # postion info for previous instructed low value target\n",
      "            #it_reward = np.append(it_reward,state[ind_check_reward_states[i]+1] == 'reward')  # reward for previous instructed low value target\n",
      "            #fc_low_side = np.append(fc_low_side,targetL_info[i][2]) # what side low value target is presented on in free choice trial\n",
      "            #fc_target = np.append(fc_target,target_all_block3)  # what target is selected: 1 = low value, 2 = high value\n",
      "            counter += 1\n",
      "\n",
      "y = np.array(fc_target)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "it_side = np.array(it_side)\n",
      "it_reward = np.array(1*it_reward)\n",
      "fc_low_side = np.array(fc_low_side)\n",
      "const_logit = np.ones(it_side.size)\n",
      "x = np.vstack((it_side,it_reward,fc_low_side))\n",
      "x = np.transpose(x)\n",
      "x = sm.add_constant(x,prepend='False')\n",
      "#x = sm.add_constant(x,prepend=\"False\")\n",
      "d = {'FC Target Selection' : fc_target, 'IT Side' : it_side, 'IT Reward' : it_reward, 'FC Low Target Side' : fc_low_side, 'Const' : const_logit}\n",
      "\n",
      "df = pd.DataFrame(d)\n",
      "\n",
      "model_glm = sm.GLM(fc_target, x, family=sm.families.Binomial())\n",
      "\n",
      "fit_glm = model_glm.fit()\n",
      "fit_glm.summary()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>Generalized Linear Model Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td>   890</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   886</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     3</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th>    <td>1.0</td>  \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -398.57</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>           <td>Mon, 23 Mar 2015</td> <th>  Deviance:          </th> <td>  797.13</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>               <td>11:47:53</td>     <th>  Pearson chi2:      </th>  <td>  890.</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>No. Iterations:</th>         <td>6</td>        <th>                     </th>     <td> </td>   \n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>const</th> <td>   -1.7810</td> <td>    0.172</td> <td>  -10.356</td> <td> 0.000</td> <td>   -2.118    -1.444</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>x1</th>    <td>   -0.0274</td> <td>    0.182</td> <td>   -0.151</td> <td> 0.880</td> <td>   -0.383     0.328</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>x2</th>    <td>    0.4815</td> <td>    0.181</td> <td>    2.658</td> <td> 0.008</td> <td>    0.126     0.837</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>x3</th>    <td>   -0.0198</td> <td>    0.182</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.376     0.337</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                 Generalized Linear Model Regression Results                  \n",
        "==============================================================================\n",
        "Dep. Variable:                      y   No. Observations:                  890\n",
        "Model:                            GLM   Df Residuals:                      886\n",
        "Model Family:                Binomial   Df Model:                            3\n",
        "Link Function:                  logit   Scale:                             1.0\n",
        "Method:                          IRLS   Log-Likelihood:                -398.57\n",
        "Date:                Mon, 23 Mar 2015   Deviance:                       797.13\n",
        "Time:                        11:47:53   Pearson chi2:                     890.\n",
        "No. Iterations:                     6                                         \n",
        "==============================================================================\n",
        "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "const         -1.7810      0.172    -10.356      0.000        -2.118    -1.444\n",
        "x1            -0.0274      0.182     -0.151      0.880        -0.383     0.328\n",
        "x2             0.4815      0.181      2.658      0.008         0.126     0.837\n",
        "x3            -0.0198      0.182     -0.109      0.913        -0.376     0.337\n",
        "==============================================================================\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "explanatory_cols = df.columns[[0,1,3,4]]\n",
      "dependent_col = df.columns[2]\n",
      "\n",
      "logit = sm.Logit(df[dependent_col], df[explanatory_cols])\n",
      "#probit_model = sm.Probit(df[dependent_col], df[explanatory_cols])\n",
      " \n",
      "# fit the model\n",
      "result = logit.fit()\n",
      "result.summary()\n",
      "#result_probit = probit_model.fit()\n",
      "#result_probit.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.447829\n",
        "         Iterations 6\n"
       ]
      },
      {
       "html": [
        "<table class=\"simpletable\">\n",
        "<caption>Logit Regression Results</caption>\n",
        "<tr>\n",
        "  <th>Dep. Variable:</th> <td>FC Target Selection</td> <th>  No. Observations:  </th>  <td>   890</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Model:</th>                <td>Logit</td>        <th>  Df Residuals:      </th>  <td>   886</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Method:</th>                <td>MLE</td>         <th>  Df Model:          </th>  <td>     3</td> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Date:</th>           <td>Mon, 23 Mar 2015</td>   <th>  Pseudo R-squ.:     </th> <td>0.008713</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Time:</th>               <td>11:48:00</td>       <th>  Log-Likelihood:    </th> <td> -398.57</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>converged:</th>            <td>True</td>         <th>  LL-Null:           </th> <td> -402.07</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th> </th>                       <td> </td>          <th>  LLR p-value:       </th>  <td>0.07170</td>\n",
        "</tr>\n",
        "</table>\n",
        "<table class=\"simpletable\">\n",
        "<tr>\n",
        "           <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
        "</tr>\n",
        "<tr>\n",
        "  <th>Const</th>              <td>   -1.7810</td> <td>    0.172</td> <td>  -10.356</td> <td> 0.000</td> <td>   -2.118    -1.444</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>FC Low Target Side</th> <td>   -0.0198</td> <td>    0.182</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.376     0.337</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>IT Reward</th>          <td>    0.4815</td> <td>    0.181</td> <td>    2.658</td> <td> 0.008</td> <td>    0.126     0.837</td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th>IT Side</th>            <td>   -0.0274</td> <td>    0.182</td> <td>   -0.151</td> <td> 0.880</td> <td>   -0.383     0.328</td>\n",
        "</tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<class 'statsmodels.iolib.summary.Summary'>\n",
        "\"\"\"\n",
        "                            Logit Regression Results                           \n",
        "===============================================================================\n",
        "Dep. Variable:     FC Target Selection   No. Observations:                  890\n",
        "Model:                           Logit   Df Residuals:                      886\n",
        "Method:                            MLE   Df Model:                            3\n",
        "Date:                 Mon, 23 Mar 2015   Pseudo R-squ.:                0.008713\n",
        "Time:                         11:48:00   Log-Likelihood:                -398.57\n",
        "converged:                        True   LL-Null:                       -402.07\n",
        "                                         LLR p-value:                   0.07170\n",
        "======================================================================================\n",
        "                         coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
        "--------------------------------------------------------------------------------------\n",
        "Const                 -1.7810      0.172    -10.356      0.000        -2.118    -1.444\n",
        "FC Low Target Side    -0.0198      0.182     -0.109      0.913        -0.376     0.337\n",
        "IT Reward              0.4815      0.181      2.658      0.008         0.126     0.837\n",
        "IT Side               -0.0274      0.182     -0.151      0.880        -0.383     0.328\n",
        "======================================================================================\n",
        "\"\"\""
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
      "model = model.fit(df[explanatory_cols], df[dependent_col])\n",
      "\n",
      "# check the accuracy on the training set\n",
      "model.score(df[explanatory_cols], df[dependent_col])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(zip(explanatory_cols, np.transpose(model.coef_)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_fcchoice_lowvalueside = stats.ttest_ind(fc_low_side,fc_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_fcchoice_lowvalueside"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}