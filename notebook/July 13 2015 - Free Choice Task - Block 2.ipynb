{
 "metadata": {
  "name": "",
  "signature": "sha256:baad9cd28f93e592452a4d979df774fa8777043245c2637e8c9be794c328c261"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Free choice task with reward probabilities 40% (yellow) and 80% (blue)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd C:\\Users\\Samantha\\Dropbox\\Carmena Lab\\Luigi\\hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C:\\Users\\Samantha\\Dropbox\\Carmena Lab\\Luigi\\hdf\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import tables\n",
      "import numpy\n",
      "from pylab import rcParams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: TkAgg\n",
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdf = tables.openFile('luig20150713_14.hdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['figure.figsize'] = 15, 5  # first parameter is width, second is height\n",
      "\n",
      "# states\n",
      "state = hdf.root.task_msgs[:]['msg']\n",
      "state_time = hdf.root.task_msgs[:]['time']\n",
      "# target info\n",
      "targetH = hdf.root.task[:]['targetH']\n",
      "targetL = hdf.root.task[:]['targetL']\n",
      "# reward schedules\n",
      "reward_scheduleH = hdf.root.task[:]['reward_scheduleH']\n",
      "reward_scheduleL = hdf.root.task[:]['reward_scheduleL']\n",
      "# instructed (1) or free-choice (2) trial \n",
      "trial_type = hdf.root.task[:]['target_index']\n",
      "\n",
      "ind_wait_states = np.ravel(np.nonzero(state == 'wait'))\n",
      "ind_target_states = np.ravel(np.nonzero(state == 'target'))\n",
      "ind_check_reward_states = np.ravel(np.nonzero(state == 'check_reward'))\n",
      "num_successful_trials = ind_check_reward_states.size\n",
      "target_times = state_time[ind_target_states]\n",
      "# creates vector same size of state vectors for comparison. instructed (0) and free-choice (1)\n",
      "instructed_or_freechoice = trial_type[state_time[ind_target_states]]\n",
      "# creates vector of same size of state vectors for comparision. (0) = small reward, (1) = large reward.\n",
      "rewarded_reward_scheduleH = reward_scheduleH[state_time[ind_target_states]]\n",
      "rewarded_reward_scheduleL = reward_scheduleL[state_time[ind_target_states]]\n",
      "num_free_choice_trials = sum(instructed_or_freechoice) - num_successful_trials\n",
      "# creates vector of same size of target info: maxtrix of num_successful_trials x 3; (position_offset, reward_prob, left/right)\n",
      "targetH_info = targetH[state_time[ind_target_states]]\n",
      "targetL_info = targetL[state_time[ind_target_states]]\n",
      "\n",
      "counter = 0\n",
      "time_counter = 0\n",
      "running_avg_length = 50\n",
      "\n",
      "# initialize variables\n",
      "target_all = np.zeros(ind_check_reward_states.size)\n",
      "reward_all = np.zeros(target_all.size)\n",
      "target_freechoice = np.zeros(num_free_choice_trials)\n",
      "reward_freechoice = np.zeros(num_free_choice_trials)\n",
      "prob_choose_high_freechoice = np.zeros(target_freechoice.size)\n",
      "prob_choose_low_freechoice = np.zeros(target_freechoice.size)\n",
      "prob_reward_high_freechoice = np.zeros(target_freechoice.size)\n",
      "prob_reward_low_freechoice = np.zeros(target_freechoice.size)\n",
      "prob_lowR_switchH = np.zeros(target_freechoice.size)\n",
      "prob_lowNR_switchH = np.zeros(target_freechoice.size)\n",
      "prob_highR_switchL = np.zeros(target_freechoice.size)\n",
      "prob_highNR_switchL = np.zeros(target_freechoice.size)\n",
      "prob_choose_high_all = np.zeros(target_all.size)\n",
      "prob_choose_low_all = np.zeros(target_all.size)\n",
      "prob_reward_high_all = np.zeros(target_all.size)\n",
      "prob_reward_low_all = np.zeros(target_all.size)\n",
      "\n",
      "'''\n",
      "Target choices for all (instructed and free-choice) and free-choice only trials\n",
      "'''\n",
      "for i in range(0,ind_check_reward_states.size):\n",
      "\ttarget_state = state[ind_check_reward_states[i] - 2]\n",
      "\t#trial = trial_type[state_time[ind_check_reward_states[i] -2]]\n",
      "\ttrial = instructed_or_freechoice[i]\n",
      "\tif target_state == 'hold_targetL':\n",
      "\t\ttarget_all[i] = 1\n",
      "\t\treward_all[i] = rewarded_reward_scheduleL[i]\n",
      "\telse:\n",
      "\t\ttarget_all[i] = 2\n",
      "\t\treward_all[i] = rewarded_reward_scheduleH[i]\n",
      "\n",
      "\t#reward_all[i] = state[ind_check_reward_states[i]+1] == 'reward'\n",
      "\tif trial == 2:\n",
      "\t\ttarget_freechoice[counter] = target_all[i]\n",
      "\t\treward_freechoice[counter] = reward_all[i]\n",
      "\t\tcounter += 1\n",
      "\n",
      "'''\n",
      "Probability of target selection, reward, and switching for free-choice trials\n",
      "'''\n",
      "for i in range(0,target_freechoice.size):\n",
      "\tchosen_high_freechoice = target_freechoice[range(np.maximum(0,i - running_avg_length),i+1)] == 2\n",
      "\tchosen_low_freechoice = target_freechoice[range(np.maximum(0,i - running_avg_length),i+1)] == 1\n",
      "\treward_high_freechoice = np.logical_and(chosen_high_freechoice,reward_freechoice[range(np.maximum(0,i - running_avg_length),i+1)])\n",
      "\treward_low_freechoice = np.logical_and(chosen_low_freechoice,reward_freechoice[range(np.maximum(0,i - running_avg_length),i+1)])\n",
      "    \n",
      "\tprob_choose_high_freechoice[i] = float(sum(chosen_high_freechoice))/np.minimum(i+1,running_avg_length)\n",
      "\tprob_choose_low_freechoice[i] = float(sum(chosen_low_freechoice))/np.minimum(i+1,running_avg_length)\n",
      "\tprob_reward_high_freechoice[i] = float(sum(reward_high_freechoice))/(sum(chosen_high_freechoice) + (sum(chosen_high_freechoice)==0))  # add logic statment to denominator so we never divide by 0\n",
      "\tprob_reward_low_freechoice[i] = float(sum(reward_low_freechoice))/(sum(chosen_low_freechoice) + (sum(chosen_low_freechoice)==0))\n",
      "    \n",
      "\t'''\n",
      "\tlowR_switchH = np.logical_and(chosen_high_freechoice[1:],reward_low_freechoice[:-1])\n",
      "\tlowNR_switchH = np.logical_and(np.logical_and(chosen_high_freechoice[1:],chosen_low_freechoice[:-1]),np.logical_not(reward_low_freechoice[:-1]))\n",
      "\thighR_switchL = np.logical_and(chosen_low_freechoice[1:],reward_high_freechoice[:-1])\n",
      "\thighNR_switchL = np.logical_and(np.logical_and(chosen_low_freechoice[1:],chosen_high_freechoice[:-1]),np.logical_not(reward_high_freechoice[:-1]))\n",
      "\n",
      "\tprob_lowR_switchH[i] = float(sum(lowR_switchH))/(sum(reward_low_freechoice[:-1]) + np.nan*(sum(reward_low_freechoice[:-1])==0))\n",
      "\tprob_lowNR_switchH[i] = float(sum(lowNR_switchH))/(sum(np.logical_and(chosen_low_freechoice[:-1],np.logical_not(reward_low_freechoice[:-1]))) + np.nan*(sum(np.logical_and(chosen_low_freechoice[:-1],np.logical_not(reward_low_freechoice[:-1])))==0))\n",
      "\tprob_highR_switchL[i] = float(sum(highR_switchL))/(sum(reward_high_freechoice[:-1]) + np.nan*(sum(reward_high_freechoice)==0))\n",
      "\tprob_highNR_switchL[i] = float(sum(highNR_switchL))/(sum(np.logical_and(chosen_high_freechoice[:-1],np.logical_not(reward_high_freechoice[:-1]))) + np.nan*(sum(np.logical_and(chosen_high_freechoice[:-1],np.logical_not(reward_high_freechoice[:-1])))==0))\n",
      "\t'''\n",
      "'''\n",
      "Probability of target select and reward for all trials\n",
      "'''\n",
      "for i in range(0,target_all.size):\n",
      "\tchosen_high_all = target_all[range(np.maximum(0,i - running_avg_length),i+1)] == 2\n",
      "\tchosen_low_all = target_all[range(np.maximum(0,i - running_avg_length),i+1)] == 1\n",
      "\treward_high_all = np.logical_and(chosen_high_all,reward_all[range(np.maximum(0,i - running_avg_length),i+1)])\n",
      "\treward_low_all = np.logical_and(chosen_low_all,reward_all[range(np.maximum(0,i - running_avg_length),i+1)])\n",
      "\n",
      "\tprob_choose_high_all[i] = float(sum(chosen_high_all))/np.minimum(i+1,running_avg_length)\n",
      "\tprob_choose_low_all[i] = float(sum(chosen_low_all))/np.minimum(i+1,running_avg_length)\n",
      "\tprob_reward_high_all[i] = float(sum(reward_high_all))/(sum(chosen_high_all) + (sum(chosen_high_all)==0))\n",
      "\tprob_reward_low_all[i] = float(sum(reward_low_all))/(sum(chosen_low_all) + (sum(chosen_low_all)==0))\n",
      "\n",
      "'''\n",
      "Plot results\n",
      "'''\n",
      "\n",
      "print \"Number of total trials:\", target_all.size\n",
      "print \"Number of free-choice trials:\", target_freechoice.size \n",
      "\n",
      "# Two subplots \n",
      "plt.figure(1)\n",
      "plt.subplot(121)\n",
      "plt.plot(range(1,target_all.size+1),prob_choose_high_all,'b',range(1,target_all.size+1),prob_choose_low_all,'r')\n",
      "plt.axis([1,target_all.size,0, 1])\n",
      "plt.xlabel('Trials')\n",
      "plt.ylabel('Probability of Target Selection')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.plot(range(1,target_freechoice.size+1),prob_choose_high_freechoice,'b',range(1,target_freechoice.size+1),prob_choose_low_freechoice,'r')\n",
      "plt.axis([1,target_freechoice.size, 0, 1])\n",
      "plt.xlabel('Trials')\n",
      "plt.savefig('Prob Target Selection.pdf')\n",
      "\n",
      "plt.figure(2)\n",
      "plt.subplot(121)\n",
      "plt.plot(range(1,target_all.size+1),prob_reward_high_all,'b',range(1,target_all.size+1),prob_reward_low_all,'r')\n",
      "plt.axis([1,target_all.size,0, 1])\n",
      "plt.xlabel('Trials')\n",
      "plt.ylabel('Probability of Receiving Large Reward')\n",
      "\n",
      "plt.subplot(122)\n",
      "plt.plot(range(1,target_freechoice.size+1),prob_reward_high_freechoice,'b',range(1,target_freechoice.size+1),prob_reward_low_freechoice,'r')\n",
      "plt.axis([1,target_freechoice.size, 0, 1])\n",
      "plt.xlabel('Trials')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of total trials: 300\n",
        "Number of free-choice trials: 161\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "<matplotlib.text.Text at 0x91f4c30>"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    }
   ],
   "metadata": {}
  }
 ]
}